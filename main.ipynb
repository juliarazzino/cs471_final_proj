{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS471 Final Project Data Processing\n",
    "### Julia Razzino, Connor Brezenski, Michaela Kovalsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are designing a machine learning model to predict an individual's Big Five personality traits based on their extended Spotify listening history. First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connorbrezenski/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function to read in the listening history data (json) and convert it to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_csv(json_filename, csv_filename=None):\n",
    "    try:\n",
    "        if csv_filename is None:\n",
    "            csv_filename = json_filename.replace('.json', '.csv')\n",
    "        \n",
    "        with open(json_filename, encoding='utf-8') as inputfile:\n",
    "            df = pd.read_json(inputfile)\n",
    "        \n",
    "        df.to_csv(csv_filename, encoding='utf-8', index=False)\n",
    "        \n",
    "        return f\"CSV file '{csv_filename}' created successfully.\"\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file '{json_filename}' does not exist.\"\n",
    "    except ValueError as e:\n",
    "        return f\"Error: {e}. Ensure the JSON structure is valid for conversion.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we call the function we just made and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'data/Kovalsky_Streaming_History_Audio_2014-2020.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "result = json_to_csv('data/Kovalsky_Streaming_History_Audio_2014-2020.json')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the Spotify Web API install and set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: spotipy in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\c25julia.razzino\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: redis>=3.5.3 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spotipy) (5.2.0)\n",
      "Requirement already satisfied: requests>=2.25.0 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spotipy) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spotipy) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\c25julia.razzino\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.25.0->spotipy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.25.0->spotipy) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\c25julia.razzino\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.25.0->spotipy) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas spotipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the spotify API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your Spotify API credentials\n",
    "CLIENT_ID = 'ff359b48e7074fe09b99477eb78af081'         # Replace with your Client ID\n",
    "CLIENT_SECRET = 'b726f74b70a54c458a59839442f6a710' # Replace with your Client Secret\n",
    "\n",
    "# Authenticate with Spotify\n",
    "client_credentials_manager = SpotifyClientCredentials(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET\n",
    ")\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path listening history file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your JSON file path\n",
    "json_filename = '.\\\\data\\\\Brezenski_Streaming_History_Audio_2023-2024_8.json'\n",
    "\n",
    "# Load the JSON data into a DataFrame\n",
    "df = pd.read_json(json_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perpare data for Spotify API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tracks: 5407\n"
     ]
    }
   ],
   "source": [
    "# Ensure the column name matches your data structure\n",
    "track_uri_column = 'spotify_track_uri'  # Adjust if your column name is different\n",
    "\n",
    "# Drop rows without a track URI\n",
    "df = df.dropna(subset=[track_uri_column])\n",
    "\n",
    "# Extract unique track URIs\n",
    "unique_track_uris = df[track_uri_column].unique().tolist()\n",
    "\n",
    "print(f\"Total unique tracks: {len(unique_track_uris)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to hold fetched data\n",
    "track_info_dict = {}\n",
    "audio_features_dict = {}\n",
    "artist_genres_dict = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call track details and audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotify API allows fetching up to 50 tracks per request\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for i in range(0, len(unique_track_uris), BATCH_SIZE):\n",
    "    batch_uris = unique_track_uris[i:i+BATCH_SIZE]\n",
    "    \n",
    "    # Fetch track details\n",
    "    try:\n",
    "        tracks = sp.tracks(batch_uris)['tracks']\n",
    "        for track in tracks:\n",
    "            if track:  # Check if track is not None\n",
    "                track_id = track['uri']\n",
    "                track_info_dict[track_id] = {\n",
    "                    'track_name': track['name'],\n",
    "                    'album_name': track['album']['name'],\n",
    "                    'album_release_date': track['album']['release_date'],\n",
    "                    'track_popularity': track['popularity'],\n",
    "                    'duration_ms': track['duration_ms'],\n",
    "                    'explicit': track['explicit'],\n",
    "                    'artist_ids': [artist['id'] for artist in track['artists']],\n",
    "                    'artist_names': [artist['name'] for artist in track['artists']],\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching track details for batch starting at index {i}: {e}\")\n",
    "    \n",
    "    # Fetch audio features\n",
    "    try:\n",
    "        features = sp.audio_features(batch_uris)\n",
    "        for feature in features:\n",
    "            if feature:  # Check if feature is not None\n",
    "                track_id = feature['uri']\n",
    "                audio_features_dict[track_id] = feature\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching audio features for batch starting at index {i}: {e}\")\n",
    "    \n",
    "    time.sleep(0.1)  # Adjust sleep time as necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch artist's generes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique artists: 3542\n"
     ]
    }
   ],
   "source": [
    "# Collect all unique artist IDs\n",
    "all_artist_ids = set()\n",
    "for track_data in track_info_dict.values():\n",
    "    all_artist_ids.update(track_data['artist_ids'])\n",
    "\n",
    "print(f\"Total unique artists: {len(all_artist_ids)}\")\n",
    "\n",
    "# Fetch artist genres in batches\n",
    "artist_ids_list = list(all_artist_ids)\n",
    "for i in range(0, len(artist_ids_list), BATCH_SIZE):\n",
    "    batch_artist_ids = artist_ids_list[i:i+BATCH_SIZE]\n",
    "    try:\n",
    "        artists = sp.artists(batch_artist_ids)['artists']\n",
    "        for artist in artists:\n",
    "            if artist:  # Check if artist is not None\n",
    "                artist_id = artist['id']\n",
    "                artist_genres_dict[artist_id] = artist['genres']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching artist genres for batch starting at index {i}: {e}\")\n",
    "    \n",
    "    time.sleep(0.1)  # Adjust sleep time as necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dictionaries to data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert track info dict to DataFrame\n",
    "track_info_df = pd.DataFrame.from_dict(track_info_dict, orient='index')\n",
    "\n",
    "# Convert audio features dict to DataFrame\n",
    "audio_features_df = pd.DataFrame.from_dict(audio_features_dict, orient='index')\n",
    "\n",
    "# Reset index to have 'spotify_track_uri' as a column\n",
    "track_info_df = track_info_df.reset_index().rename(columns={'index': 'spotify_track_uri'})\n",
    "audio_features_df = audio_features_df.reset_index().rename(columns={'index': 'spotify_track_uri'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         spotify_track_uri  \\\n",
      "0     spotify:track:7aqfrAY2p9BUSiupwk3svU   \n",
      "1     spotify:track:7LTCmtMpf3JPfBE8gAl64z   \n",
      "2     spotify:track:0VJXWgw0GWsprapvlpbuyu   \n",
      "3     spotify:track:4M68xjcc42oxyphhzpOWXS   \n",
      "4     spotify:track:5VE6OSfJkMHyMpHaJzuUqn   \n",
      "...                                    ...   \n",
      "5402  spotify:track:688ucfDoelAC8sY7wgHCV4   \n",
      "5403  spotify:track:2wAJTrFhCnQyNSD3oUgTZO   \n",
      "5404  spotify:track:3mTpegrOwRn0oJjv4TSbEE   \n",
      "5405  spotify:track:6gbiTbclnHlmSIPfmF2zEc   \n",
      "5406  spotify:track:5A8xI7PN4WDe9e61xEdt94   \n",
      "\n",
      "                                track_name                      album_name  \\\n",
      "0     First Person Shooter (feat. J. Cole)                For All The Dogs   \n",
      "1           Turn Yo Clic Up (feat. Future)  Turn Yo Clic Up (feat. Future)   \n",
      "2                                     Okay                            Okay   \n",
      "3                             Great Gatsby                       Nostalgia   \n",
      "4                            Gimme Da Lite                   Gimme Da Lite   \n",
      "...                                    ...                             ...   \n",
      "5402                      Jesus in my life                Jesus in my life   \n",
      "5403                              Work Out  Cole World: The Sideline Story   \n",
      "5404                      Can't Get Enough  Cole World: The Sideline Story   \n",
      "5405                Gang Shit No Lame Shit                        Glockoma   \n",
      "5406                             Yeah Glo!               Ehhthang Ehhthang   \n",
      "\n",
      "     album_release_date  track_popularity  duration_ms  explicit  \\\n",
      "0            2023-10-06                74       247444      True   \n",
      "1            2023-07-14                58       230835      True   \n",
      "2            2023-12-08                43       192195      True   \n",
      "3            2023-09-15                69       146751      True   \n",
      "4            2023-12-08                55       113333      True   \n",
      "...                 ...               ...          ...       ...   \n",
      "5402         2022-02-04                41       147719     False   \n",
      "5403         2011-09-27                79       235320      True   \n",
      "5404         2011-09-27                69       225960     False   \n",
      "5405         2018-11-23                72       131600      True   \n",
      "5406         2024-04-05                75       142702      True   \n",
      "\n",
      "                                             artist_ids  \\\n",
      "0      [3TVXtAsR1Inumwj472S9r4, 6l3HvQ5sa6mXTsMTB19rO5]   \n",
      "1      [0VRj0yCOv2FXJNP47XQnx5, 1RyvyyTE3xzB2ZywiAwp0i]   \n",
      "2     [6vXTefBL93Dj5IqAWq6OTv, 5f7VJjfbwm532GiveGC0Z...   \n",
      "3                              [45TgXXqMDdF8BkjA83OM7z]   \n",
      "4      [23DYJsw4uSCguIqiTIDtcN, 6icQOAFXDZKsumw3YXyusw]   \n",
      "...                                                 ...   \n",
      "5402                           [1KTljUXZGt7HkAFFEnDBn1]   \n",
      "5403                           [6l3HvQ5sa6mXTsMTB19rO5]   \n",
      "5404   [6l3HvQ5sa6mXTsMTB19rO5, 2iojnBLj0qIMiKPvVhLnsH]   \n",
      "5405                           [0RESbWvOMyua0yuyVrztJ5]   \n",
      "5406                           [2qoQgPAilErOKCwE2Y8wOG]   \n",
      "\n",
      "                               artist_names  \n",
      "0                          [Drake, J. Cole]  \n",
      "1                           [Quavo, Future]  \n",
      "2     [French Montana, Lil Baby, ATL Jacob]  \n",
      "3                                [Rod Wave]  \n",
      "4                   [Southside, Lil Yachty]  \n",
      "...                                     ...  \n",
      "5402                            [Skema Boy]  \n",
      "5403                              [J. Cole]  \n",
      "5404                  [J. Cole, Trey Songz]  \n",
      "5405                            [Key Glock]  \n",
      "5406                             [GloRilla]  \n",
      "\n",
      "[5407 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(track_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge results into one data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge track info and audio features on 'spotify_track_uri'\n",
    "track_data_df = pd.merge(track_info_df, audio_features_df, on='spotify_track_uri', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review columns for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['spotify_track_uri', 'track_name', 'album_name', 'album_release_date',\n",
      "       'track_popularity', 'duration_ms_x', 'explicit', 'artist_ids',\n",
      "       'artist_names', 'danceability', 'energy', 'key', 'loudness', 'mode',\n",
      "       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
      "       'valence', 'tempo', 'type', 'id', 'uri', 'track_href', 'analysis_url',\n",
      "       'duration_ms_y', 'time_signature'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(track_data_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get genres for artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get genres for a list of artist IDs\n",
    "def get_genres_for_artists(artist_ids):\n",
    "    genres = set()\n",
    "    for artist_id in artist_ids:\n",
    "        artist_genres = artist_genres_dict.get(artist_id, [])\n",
    "        genres.update(artist_genres)\n",
    "    return list(genres)\n",
    "\n",
    "# Apply the function to each row\n",
    "track_data_df['artist_genres'] = track_data_df['artist_ids'].apply(get_genres_for_artists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten 'artist_ids' and 'artist_names' to strings\n",
    "track_data_df['artist_ids'] = track_data_df['artist_ids'].apply(lambda x: ', '.join(x))\n",
    "track_data_df['artist_names'] = track_data_df['artist_names'].apply(lambda x: ', '.join(x))\n",
    "track_data_df['artist_genres'] = track_data_df['artist_genres'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Optionally, drop columns you don't need\n",
    "# track_data_df = track_data_df.drop(columns=['type', 'id', 'track_href', 'analysis_url', 'time_signature'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data into data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the enriched track data back into the original DataFrame\n",
    "df_enriched = pd.merge(df, track_data_df, on='spotify_track_uri', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ts    username platform  ms_played conn_country  \\\n",
      "0      2023-12-22T15:58:39Z  kingzenski      ios     109100           US   \n",
      "1      2023-12-22T15:58:40Z  kingzenski      ios       1010           US   \n",
      "2      2023-12-22T15:58:56Z  kingzenski      ios      16620           US   \n",
      "3      2023-12-22T15:59:00Z  kingzenski      ios       4050           US   \n",
      "4      2023-12-22T15:59:06Z  kingzenski      ios       5700           US   \n",
      "...                     ...         ...      ...        ...          ...   \n",
      "16909  2024-05-31T22:45:15Z  kingzenski      ios      34943           US   \n",
      "16910  2024-05-31T22:45:19Z  kingzenski      ios       3370           US   \n",
      "16911  2024-05-31T22:45:22Z  kingzenski      ios       3541           US   \n",
      "16912  2024-05-31T22:45:25Z  kingzenski      ios       2560           US   \n",
      "16913  2024-05-31T22:45:29Z  kingzenski      ios       3754           US   \n",
      "\n",
      "                           ip_addr_decrypted user_agent_decrypted  \\\n",
      "0                             172.58.125.201              unknown   \n",
      "1                             172.58.125.201              unknown   \n",
      "2                             172.58.125.201              unknown   \n",
      "3                             172.58.125.201              unknown   \n",
      "4                             172.58.125.201              unknown   \n",
      "...                                      ...                  ...   \n",
      "16909  2607:fb91:7c6:962c:352f:6466:ead:8521                 None   \n",
      "16910  2607:fb91:7c6:962c:352f:6466:ead:8521                 None   \n",
      "16911  2607:fb91:7c6:962c:352f:6466:ead:8521                 None   \n",
      "16912  2607:fb91:7c6:962c:352f:6466:ead:8521                 None   \n",
      "16913  2607:fb91:7c6:962c:352f:6466:ead:8521                 None   \n",
      "\n",
      "                 master_metadata_track_name master_metadata_album_artist_name  \\\n",
      "0      First Person Shooter (feat. J. Cole)                             Drake   \n",
      "1            Turn Yo Clic Up (feat. Future)                             Quavo   \n",
      "2                                      Okay                    French Montana   \n",
      "3                              Great Gatsby                          Rod Wave   \n",
      "4                             Gimme Da Lite                         Southside   \n",
      "...                                     ...                               ...   \n",
      "16909    Roll in Peace (feat. XXXTENTACION)                       Kodak Black   \n",
      "16910                        Jocelyn Flores                      XXXTENTACION   \n",
      "16911                            Time Today                      Moneybagg Yo   \n",
      "16912                     F.N.F. (Let's Go)                           Hitkidd   \n",
      "16913                              Ultimate                      Denzel Curry   \n",
      "\n",
      "      master_metadata_album_album_name  ... valence    tempo            type  \\\n",
      "0                     For All The Dogs  ...   0.245  163.997  audio_features   \n",
      "1       Turn Yo Clic Up (feat. Future)  ...   0.629  124.927  audio_features   \n",
      "2                                 Okay  ...   0.284  123.065  audio_features   \n",
      "3                            Nostalgia  ...   0.154  156.975  audio_features   \n",
      "4                        Gimme Da Lite  ...   0.406  144.014  audio_features   \n",
      "...                                ...  ...     ...      ...             ...   \n",
      "16909     Project Baby 2: All Grown Up  ...   0.363  140.053  audio_features   \n",
      "16910                               17  ...   0.437  134.021  audio_features   \n",
      "16911                 A Gangstaâ€™s Pain  ...   0.327  136.994  audio_features   \n",
      "16912                F.N.F. (Let's Go)  ...   0.647  157.144  audio_features   \n",
      "16913                         Ultimate  ...   0.750  112.865  audio_features   \n",
      "\n",
      "                           id                                   uri  \\\n",
      "0      7aqfrAY2p9BUSiupwk3svU  spotify:track:7aqfrAY2p9BUSiupwk3svU   \n",
      "1      7LTCmtMpf3JPfBE8gAl64z  spotify:track:7LTCmtMpf3JPfBE8gAl64z   \n",
      "2      0VJXWgw0GWsprapvlpbuyu  spotify:track:0VJXWgw0GWsprapvlpbuyu   \n",
      "3      4M68xjcc42oxyphhzpOWXS  spotify:track:4M68xjcc42oxyphhzpOWXS   \n",
      "4      5VE6OSfJkMHyMpHaJzuUqn  spotify:track:5VE6OSfJkMHyMpHaJzuUqn   \n",
      "...                       ...                                   ...   \n",
      "16909  40oKW22ZNNkEdZLJTScaQI  spotify:track:40oKW22ZNNkEdZLJTScaQI   \n",
      "16910  7m9OqQk4RVRkw9JJdeAw96  spotify:track:7m9OqQk4RVRkw9JJdeAw96   \n",
      "16911  0YrKSfpvflYnLSBd4Jtu6J  spotify:track:0YrKSfpvflYnLSBd4Jtu6J   \n",
      "16912  1vrFJDrysqmsNAgyjBzx4f  spotify:track:1vrFJDrysqmsNAgyjBzx4f   \n",
      "16913  6R0GRYk2vs2XuBVemYK5YZ  spotify:track:6R0GRYk2vs2XuBVemYK5YZ   \n",
      "\n",
      "                                              track_href  \\\n",
      "0      https://api.spotify.com/v1/tracks/7aqfrAY2p9BU...   \n",
      "1      https://api.spotify.com/v1/tracks/7LTCmtMpf3JP...   \n",
      "2      https://api.spotify.com/v1/tracks/0VJXWgw0GWsp...   \n",
      "3      https://api.spotify.com/v1/tracks/4M68xjcc42ox...   \n",
      "4      https://api.spotify.com/v1/tracks/5VE6OSfJkMHy...   \n",
      "...                                                  ...   \n",
      "16909  https://api.spotify.com/v1/tracks/40oKW22ZNNkE...   \n",
      "16910  https://api.spotify.com/v1/tracks/7m9OqQk4RVRk...   \n",
      "16911  https://api.spotify.com/v1/tracks/0YrKSfpvflYn...   \n",
      "16912  https://api.spotify.com/v1/tracks/1vrFJDrysqms...   \n",
      "16913  https://api.spotify.com/v1/tracks/6R0GRYk2vs2X...   \n",
      "\n",
      "                                            analysis_url  duration_ms_y  \\\n",
      "0      https://api.spotify.com/v1/audio-analysis/7aqf...         247444   \n",
      "1      https://api.spotify.com/v1/audio-analysis/7LTC...         230836   \n",
      "2      https://api.spotify.com/v1/audio-analysis/0VJX...         192195   \n",
      "3      https://api.spotify.com/v1/audio-analysis/4M68...         146752   \n",
      "4      https://api.spotify.com/v1/audio-analysis/5VE6...         113333   \n",
      "...                                                  ...            ...   \n",
      "16909  https://api.spotify.com/v1/audio-analysis/40oK...         213132   \n",
      "16910  https://api.spotify.com/v1/audio-analysis/7m9O...         119133   \n",
      "16911  https://api.spotify.com/v1/audio-analysis/0YrK...         136900   \n",
      "16912  https://api.spotify.com/v1/audio-analysis/1vrF...         137580   \n",
      "16913  https://api.spotify.com/v1/audio-analysis/6R0G...         189974   \n",
      "\n",
      "       time_signature                                      artist_genres  \n",
      "0                   4  north carolina hip hop, rap, hip hop, canadian...  \n",
      "1                   4  rap, atl hip hop, melodic rap, hip hop, trap, ...  \n",
      "2                   4  rap, atl hip hop, hip hop, pop rap, atl trap, ...  \n",
      "3                   4                                        florida rap  \n",
      "4                   4      rap, atl hip hop, melodic rap, atl trap, trap  \n",
      "...               ...                                                ...  \n",
      "16909               4  miami hip hop, florida drill, rap, florida rap...  \n",
      "16910               4                        rap, miami hip hop, emo rap  \n",
      "16911               4  rap, tennessee hip hop, memphis hip hop, trap,...  \n",
      "16912               4                  southern hip hop, memphis hip hop  \n",
      "16913               4  miami hip hop, drill, rap, underground hip hop...  \n",
      "\n",
      "[16914 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched data saved to data/your_streaming_history_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "# Replace with your desired output file path\n",
    "output_csv_filename = 'data/your_streaming_history_enriched.csv'\n",
    "\n",
    "# Save to CSV\n",
    "df_enriched.to_csv(output_csv_filename, index=False)\n",
    "\n",
    "print(f\"Enriched data saved to {output_csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Kovalsky_2014-2020.csv\n",
      "Extracted username: Kovalsky\n",
      "Processing file: Razzino_2023-2024.csv\n",
      "Extracted username: Razzino\n",
      "Processing file: Blalock_2023-2024.csv\n",
      "Extracted username: Blalock\n",
      "Processing file: results.csv\n",
      "Extracted username: \n",
      "Processing file: Brezenski_2023-2024.csv\n",
      "Extracted username: Brezenski\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/s8x7jmb54wq7ldv0c_5hb_wm0000gn/T/ipykernel_57820/3167703484.py:23: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_user = pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = 'data/processed/'  # Replace with your actual directory path\n",
    "\n",
    "# Get a list of all CSV files\n",
    "csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
    "\n",
    "# Initialize a list to hold data from each user\n",
    "user_data_list = []\n",
    "\n",
    "for file in csv_files:\n",
    "    # Extract the username from the filename\n",
    "    filename = os.path.basename(file)\n",
    "    name_part = os.path.splitext(filename)[0]\n",
    "    name_parts = name_part.split('_')\n",
    "    username = '_'.join(name_parts[:-1])  # Joins all parts except the last one (assuming date is last)\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df_user = pd.read_csv(file)\n",
    "    \n",
    "    # Add the username column\n",
    "    df_user['username'] = username\n",
    "    \n",
    "    # Debugging: Print out the filename and extracted username\n",
    "    print(f'Processing file: {filename}')\n",
    "    print(f'Extracted username: {username}')\n",
    "    \n",
    "    # Append to the list\n",
    "    user_data_list.append(df_user)\n",
    "\n",
    "# Combine all user data into a single DataFrame\n",
    "df = pd.concat(user_data_list, ignore_index=True)\n",
    "\n",
    "# Proceed with the rest of your data processing...\n",
    "assert 'username' in df.columns, \"'username' column is missing in df\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts                                       3\n",
      "username                                 0\n",
      "platform                                 3\n",
      "ms_played                                3\n",
      "conn_country                             3\n",
      "ip_addr_decrypted                     1780\n",
      "user_agent_decrypted                 10980\n",
      "master_metadata_track_name               3\n",
      "master_metadata_album_artist_name        3\n",
      "master_metadata_album_album_name         3\n",
      "spotify_track_uri                        3\n",
      "episode_name                         58128\n",
      "episode_show_name                    58128\n",
      "spotify_episode_uri                  58128\n",
      "reason_start                            25\n",
      "reason_end                            1796\n",
      "shuffle                                  3\n",
      "skipped                               3799\n",
      "offline                                  3\n",
      "offline_timestamp                        3\n",
      "incognito_mode                           3\n",
      "track_name                             161\n",
      "album_name                             161\n",
      "album_release_date                       3\n",
      "track_popularity                         3\n",
      "duration_ms_x                            3\n",
      "explicit                                 3\n",
      "artist_ids                               3\n",
      "artist_names                           161\n",
      "danceability                             5\n",
      "energy                                   5\n",
      "key                                      5\n",
      "loudness                                 5\n",
      "mode                                     5\n",
      "speechiness                              5\n",
      "acousticness                             5\n",
      "instrumentalness                         5\n",
      "liveness                                 5\n",
      "valence                                  5\n",
      "tempo                                    5\n",
      "type                                     5\n",
      "id                                       5\n",
      "uri                                      5\n",
      "track_href                               5\n",
      "analysis_url                             5\n",
      "duration_ms_y                            5\n",
      "time_signature                           5\n",
      "artist_genres                         4606\n",
      "PersonID                             58125\n",
      "Name                                 58125\n",
      "Openness                             58125\n",
      "Conscientiousness                    58125\n",
      "Extraversion                         58125\n",
      "Agreeableness                        58125\n",
      "Neuroticism                          58125\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Decide on a strategy: drop or impute missing values\n",
    "#df = df.dropna(subset=['essential_column1', 'essential_column2'])  # Replace with actual column names\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "irrelevant_columns = ['ts', 'platform', 'conn_country', 'ip_addr_decrypted', 'user_agent_decrypted',\n",
    "                      'spotify_track_uri', 'episode_name', 'episode_show_name', 'spotify_episode_uri',\n",
    "                      'reason_start', 'reason_end', 'offline_timestamp', 'track_href', 'analysis_url',\n",
    "                      'uri', 'id', 'track_name', 'album_name', 'album_release_date', 'duration_ms_y',\n",
    "                      'type', 'master_metadata_track_name', 'master_metadata_album_artist_name',\n",
    "                      'master_metadata_album_album_name']\n",
    "df = df.drop(columns=irrelevant_columns, errors='ignore')  # Use errors='ignore' in case some columns are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'artist_genres'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'artist_genres'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLabelBinarizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assume 'df' is your DataFrame containing all user data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. Handle missing values in 'artist_genres'\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist_genres\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martist_genres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2. Split genres into lists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist_genres_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist_genres\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'artist_genres'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Assume 'df' is your DataFrame containing all user data\n",
    "\n",
    "# 1. Handle missing values in 'artist_genres'\n",
    "df['artist_genres'] = df['artist_genres'].fillna('')\n",
    "\n",
    "# 2. Split genres into lists\n",
    "df['artist_genres_list'] = df['artist_genres'].apply(\n",
    "    lambda x: x.split(', ') if isinstance(x, str) and x else []\n",
    ")\n",
    "\n",
    "# 3. Verify that all entries in 'artist_genres_list' are lists\n",
    "assert df['artist_genres_list'].apply(lambda x: isinstance(x, list)).all(), \"Not all entries are lists.\"\n",
    "\n",
    "# 4. Aggregate genres per user\n",
    "user_genres = df.groupby('username')['artist_genres_list'].apply(\n",
    "    lambda lists: [genre for sublist in lists for genre in sublist]\n",
    ")\n",
    "\n",
    "# 5. Apply MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_dummies = pd.DataFrame(\n",
    "    mlb.fit_transform(user_genres),\n",
    "    columns=mlb.classes_,\n",
    "    index=user_genres.index\n",
    ").reset_index()\n",
    "\n",
    "print(\"'username' in genre_dummies columns:\", 'username' in genre_dummies.columns)\n",
    "print(\"Columns in genre_dummies:\", genre_dummies.columns.tolist())\n",
    "\n",
    "# 6. Handle 'key' variable\n",
    "\n",
    "# a. Fill missing 'key' values and convert to integers\n",
    "df['key'] = df['key'].fillna(-1).astype(int)\n",
    "\n",
    "# b. Create dummy variables\n",
    "key_dummies = pd.get_dummies(df['key'], prefix='key')\n",
    "\n",
    "# c. Concatenate 'username' and key_dummies\n",
    "df_keys = pd.concat([df[['username']], key_dummies], axis=1)\n",
    "\n",
    "# d. Aggregate 'key' dummies per user\n",
    "user_keys = df_keys.groupby('username').mean().reset_index()\n",
    "\n",
    "print(\"'username' in user_keys columns:\", 'username' in user_keys.columns)\n",
    "\n",
    "# 7. Combine all user-level features\n",
    "\n",
    "# a. Aggregate other numerical features per user\n",
    "agg_functions = {\n",
    "    'ms_played': 'sum',\n",
    "    'skipped': 'mean',\n",
    "    'shuffle': 'mean',\n",
    "    # Add other features as needed\n",
    "}\n",
    "\n",
    "user_numerical_agg = df.groupby('username').agg(agg_functions).reset_index()\n",
    "\n",
    "# Prepare DataFrames\n",
    "def prepare_dataframe(df, name):\n",
    "    if 'username' not in df.columns:\n",
    "        df.reset_index(inplace=True)\n",
    "    df['username'] = df['username'].astype(str)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(f\"DataFrame '{name}' columns:\", df.columns.tolist())\n",
    "    print(f\"Number of rows in '{name}':\", len(df))\n",
    "    return df\n",
    "\n",
    "user_numerical_agg = prepare_dataframe(user_numerical_agg, 'user_numerical_agg')\n",
    "genre_dummies = prepare_dataframe(genre_dummies, 'genre_dummies')\n",
    "user_keys = prepare_dataframe(user_keys, 'user_keys')\n",
    "\n",
    "# Check for NaN and duplicates\n",
    "for name, df in [('user_numerical_agg', user_numerical_agg), \n",
    "                 ('genre_dummies', genre_dummies), \n",
    "                 ('user_keys', user_keys)]:\n",
    "    print(f\"NaN in 'username' of {name}:\", df['username'].isna().sum())\n",
    "    print(f\"Duplicates in 'username' of {name}:\", df['username'].duplicated().sum())\n",
    "\n",
    "# Merge DataFrames\n",
    "user_data = user_numerical_agg.copy()\n",
    "user_data = pd.merge(user_data, genre_dummies, on='username', how='left')\n",
    "user_data = pd.merge(user_data, user_keys, on='username', how='left')\n",
    "\n",
    "# Fill missing values and infer data types\n",
    "user_data.fillna(0, inplace=True)\n",
    "user_data = user_data.infer_objects()\n",
    "\n",
    "# Merge with personality data\n",
    "user_personality = pd.read_csv('data/processed/results.csv')\n",
    "\n",
    "# Check the columns in user_personality\n",
    "print(\"Columns in user_personality:\", user_personality.columns.tolist())\n",
    "\n",
    "# Rename the column if 'username' is not present\n",
    "if 'username' not in user_personality.columns:\n",
    "    # Assuming the username is stored in the 'name' column\n",
    "    user_personality.rename(columns={'name': 'username'}, inplace=True)\n",
    "    print(\"Renamed 'name' column to 'username'.\")\n",
    "\n",
    "# Verify that 'username' is now in the columns\n",
    "print(\"'username' in user_personality columns:\", 'username' in user_personality.columns)\n",
    "print(\"Columns in user_personality after renaming:\", user_personality.columns.tolist())\n",
    "\n",
    "# Convert 'username' to string\n",
    "user_personality['username'] = user_personality['username'].astype(str)\n",
    "\n",
    "# Merge with user_data\n",
    "user_data = pd.merge(user_data, user_personality, on='username', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Not allowed to merge between different levels. (2 levels on the left, 1 on the right)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m user_keys\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m missing in user_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Merge DataFrames on 'username'\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m user_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_agg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenre_dummies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musername\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m user_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(user_data, user_keys, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/merge.py:784\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m!=\u001b[39m _right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels:\n\u001b[1;32m    779\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot allowed to merge between different levels. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m levels on the left, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on the right)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m     )\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_left_right_on(left_on, right_on)\n\u001b[1;32m    788\u001b[0m (\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     right_drop,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n",
      "\u001b[0;31mMergeError\u001b[0m: Not allowed to merge between different levels. (2 levels on the left, 1 on the right)"
     ]
    }
   ],
   "source": [
    "# Define the aggregation functions for each feature\n",
    "agg_functions = {\n",
    "    'ms_played': 'sum',\n",
    "    'skipped': 'mean',\n",
    "    'shuffle': 'mean',\n",
    "    'offline': 'mean',\n",
    "    'incognito_mode': 'mean',\n",
    "    'track_popularity': ['mean', 'std'],\n",
    "    'explicit': 'mean',\n",
    "    'danceability': ['mean', 'std'],\n",
    "    'energy': ['mean', 'std'],\n",
    "    'loudness': ['mean', 'std'],\n",
    "    'mode': 'mean',\n",
    "    'speechiness': ['mean', 'std'],\n",
    "    'acousticness': ['mean', 'std'],\n",
    "    'instrumentalness': ['mean', 'std'],\n",
    "    'liveness': ['mean', 'std'],\n",
    "    'valence': ['mean', 'std'],\n",
    "    'tempo': ['mean', 'std'],\n",
    "    'time_signature': ['mean', 'std']\n",
    "}\n",
    "\n",
    "# Aggregate per user and reset index\n",
    "user_agg = df.groupby('username').agg(agg_functions).reset_index()\n",
    "\n",
    "# Ensure 'username' is present in all DataFrames\n",
    "assert 'username' in user_agg.columns, \"'username' missing in user_agg\"\n",
    "assert 'username' in genre_dummies.columns, \"'username' missing in genre_dummies\"\n",
    "assert 'username' in user_keys.columns, \"'username' missing in user_keys\"\n",
    "\n",
    "# Merge DataFrames on 'username'\n",
    "user_data = pd.merge(user_agg, genre_dummies, on='username', how='left')\n",
    "user_data = pd.merge(user_data, user_keys, on='username', how='left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
